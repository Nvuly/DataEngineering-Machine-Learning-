{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\human\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1번 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMDB 데이터 가져오기\n",
    "\n",
    "# 단어의 수 설정 : 빈도수 상위 1부터 1000에 해당하는 단어만 선택\n",
    "vocab = 1000\n",
    "\n",
    "# 데이터 다운로드 받기\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab)\n",
    "\n",
    "### 데이터 불러오기\n",
    "\n",
    "file_path = '/content/drive/MyDrive/KDT_데이터/딥러닝/자연어처리/imdb.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df)\n",
    "\n",
    "#review를 가지고 토큰화를 하고 단어사전을 만들것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2번 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습용, 평가용 리뷰 데이터 --> 길이를 일정하게 맞춰주기\n",
    "# truncate 길이를 줄이다\n",
    "# 데이터를 순차데이터라고 하는 특성으로 접근하고 있다. 머신러닝하고는 다르다.\n",
    "'''\n",
    "tf.keras.utils.pad_sequences(data, maxlen)\n",
    "'''\n",
    "# 최대 길이 설정\n",
    "max_len=470\n",
    "\n",
    "# 자르고 붙이기\n",
    "X_train_seq = tf.keras.utils.pad_sequences(sequences=X_train, maxlen=max_len)\n",
    "\n",
    "X_test_seq = tf.keras.utils.pad_sequences(sequences=X_test, maxlen=max_len)\n",
    "\n",
    "# 결과 확인하기\n",
    "print(f'학습용 데이터의 길이를 일정하게 처리한 결과(모양) : {X_train_seq.shape}')\n",
    "print('*'*80)\n",
    "print(f'평가용 데이터의 길이를 일정하게 처리한 결과(모양) : {X_test_seq.shape}')\n",
    "print('*'*80)\n",
    "print(f'학습용 데이터 1개에 대한 처리 결과 확인 : \\n{X_train_seq[77, :]}')\n",
    "print('*'*80)\n",
    "\n",
    "\n",
    "### 공식 문서 상에 pre라 돼있는건 전을 채우던 이후를 채우던\n",
    "### 결국 상관없다 0의 위치에 신경쓰지말라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3번 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습 모델 구성하기\n",
    "\n",
    "'''\n",
    "모델 구성 순서\n",
    "1. Sequential()를 이용하여 모델 껍질(컨데이너) 생성하기\n",
    "2. Embedding layer 추가하기\n",
    "3. SimpleRNN layer 추가하기\n",
    "4. Dense layer 추가하가\n",
    "'''\n",
    "\n",
    "# 기본 설정\n",
    "vocab_size=1000\n",
    "embedding_size=32\n",
    "max_length = 470\n",
    "# 모델 구조 생성하기\n",
    "\n",
    "loaded_model = tf.keras.Sequential() #빈껍질\n",
    "\n",
    "\n",
    "loaded_model.add(tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                    output_dim=embedding_size,\n",
    "                                    input_length = max_length,\n",
    "                                    embeddings_initializer=initializer1,\n",
    "\n",
    "))\n",
    "\n",
    "loaded_model.add(tf.keras.layers.SimpleRNN(units=16,\n",
    "                                    kernel_initializer=initializer1,\n",
    "                                    recurrent_initializer=initializer2))\n",
    "\n",
    "loaded_model.add(tf.keras.layers.Dense(units=1,\n",
    "                         activation='sigmoid',\n",
    "                         kernel_initializer=initializer1))\n",
    "\n",
    "\n",
    "\n",
    "# 모델 확인하기\n",
    "loaded_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4번 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습 모델 평가\n",
    "\n",
    "# 학습 모델 불러오기\n",
    "file_path = 'rnn.h5' #content파일안에 들어있기 때문에 따로 파일패스정하지않아도 알아서 가져옴.\n",
    "# model checkpoint를 쓰면 자동저장. 반대로 안쓰면 어떻게 될까?\n",
    "# 학습된 걸 또 별도로 저장해야함.\n",
    "# 하지만 우린 위에 ModelCheckpoint를 썼기 때문에 자동저장이 됨.\n",
    "\n",
    "# load_weights가 불러오는 함수.\n",
    "# Loads all layer weights from a saved files 모든 가중치를 받겟다.\n",
    "# 학습된 모델을 불러오는게 가능하다..!\n",
    "\n",
    "loaded_model.load_weights(file_path) #여기서의 모델은 model.Sequential(무지개떡)\n",
    "\n",
    "\n",
    "\n",
    "# 항상 모델을 사용하기전에는 컴파일을 해야한다!\n",
    "\n",
    "# 모델 컴파일\n",
    "loaded_model.compile(loss='binary_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "# metrics는 여러개 넣을 수 있어서 리스트처리를 하는데 loss과 optimizer는 각각 1개씩만.\n",
    "# 문자열의 형태로만\n",
    "\n",
    "\n",
    "# 딥러닝에선 predict와 evalation을 한번에 시켜주는 함수가 있다!\n",
    "\n",
    "# 바로 evaluate이다!\n",
    "\n",
    "# 평가하기 --> evaluate() 사용!\n",
    "result = loaded_model.evaluate(x=X_test_seq, y=y_test)\n",
    "# 학습하듯이 X와 Y를 넣어준다. 머신러닝의 predict(정답,예측)과 헷갈리지말자!\n",
    "# batchsize는 따로 정의하지 않으면 32개로 맞춰진다(기본값)\n",
    "\n",
    "# 결과 확인하기\n",
    "print(result)\n",
    "\n",
    "# 앞이 손실, 뒤에가 정확도.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NvulyTarget",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
