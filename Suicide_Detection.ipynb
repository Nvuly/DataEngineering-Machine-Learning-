{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #정규 표현식\n",
    "import nltk #자연어처리\n",
    "from nltk.corpus import stopwords #불용어처리\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:\\Code\\우울증 예측(텍스트 분류 모델)/Suicide_Detection.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f'전체 데이터프레임 확인 : \\n{df}')\n",
    "\n",
    "print('*'*80)\n",
    "\n",
    "print('기본 정보 확인하기')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt') ##문장부호\n",
    "nltk.download('stopwords') ##불용어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 정의\n",
    "\n",
    "def preprocessing(text, remove_stopwords = False) : \n",
    "    # 불용어 제거는 옵션으로\n",
    "    # 1. 영어 알파벳을 제외한 나머지 문자, 공백, 숫자 등을 공백(' ')으로 대체\n",
    "    text = re.sub('[^a-zA-Z]',' ', text) \n",
    "    #문자열치환 : re.sub(pattern, replace, text), text 중 pattern에 해당하는 부분을 replace로 대체한다.\n",
    "    # 2. 대문자 --> 소문자로 변환 + nltk.word_tokenize() 함수로 토큰화\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    # 3. 불용어 제거\n",
    "    if remove_stopwords : \n",
    "        # 불용어 리스트 생성\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # 불용어가 아닌 단어(토큰)들로 이루어진 새로운 리스트 생성\n",
    "        clean_words = []\n",
    "        for word in words :\n",
    "            if word not in stopwords_list :\n",
    "                clean_words.append(word)\n",
    "                # join(list --> str 문장 붙이기) 불용어 제거\n",
    "                clean_text = ' '.join(clean_words)\n",
    "    else :\n",
    "        # join(list --> str 문장 붙이기) 불용어 포함\n",
    "        clean_text = ' '.join(words)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, remove_stopwords = False) :\n",
    "    text = re.sub('a-zA-z', ' ', text)\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    if remove_stopwords :\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        clean_words = []\n",
    "        for word in words :\n",
    "            if word not in stopwords_list :\n",
    "                clean_words.append(word)\n",
    "                clean_text = ' '.join(clean_words)\n",
    "    else :\n",
    "        clean_text = ' '.join(words)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- 함수 정의\n",
    "\n",
    "`def` 함수를 정의 , text 변수에 `re.sub` 사용해 영어 알파벳을 제외한 나머지를 공백으로 대체 합니다.\n",
    "\n",
    "- 토큰화\n",
    "\n",
    "words 변수에 `nltk.word_tokenized(text.lower())` 대문자를 소문자로 변환 하면서 토큰화를 진행 합니다.\n",
    "\n",
    "- 불용어\n",
    "\n",
    "stopwords_list 변수에 `stopowrds.words(’english’)` 사용해 불용어를 생성하고, `for`이용해 토큰화를 했던 words 변수를 word 변수에 담아서 `if`(True, False) 이용해 불용어가 없는 토큰화는 `clean_words.append(word)` clean_words 변수에 넣습니다.\n",
    "\n",
    "- 토큰화 —> 문자열 변환\n",
    "\n",
    "clean_text 변수에 불용어가 제거하고 문자열로 붙일 때 `' '.join(clean_words)` 사용 합니다.\n",
    "\n",
    "불용어를 제거 하지 않고 문자열로 붙일 때\n",
    "\n",
    "`' '.join(words)` 사용 합니다.\n",
    "\n",
    "`return` 으로 clean_text 변수를 반환 합니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text 컬럼에 대해서 전처리 함수 실행\n",
    "df.text.apply(lambda x : preprocessing(x, remove_stopword=True)) #####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rollbackTarget",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
